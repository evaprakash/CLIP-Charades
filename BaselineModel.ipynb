{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaselineModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "****BASELINE MODEL****"
      ],
      "metadata": {
        "id": "7f8YvWGEEfOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Mount to Google Drive*** (adapted from CS231n)"
      ],
      "metadata": {
        "id": "JkDN1qgXLwkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqVDLt3NLv9p",
        "outputId": "38bdf515-d88a-4189-f66a-b924ef0e7021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDERNAME = 'CS230 Project'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\""
      ],
      "metadata": {
        "id": "0D2f2hipMhXO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ],
      "metadata": {
        "id": "JWwZBdOiMsyR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load Video Frames***"
      ],
      "metadata": {
        "id": "g0PirllPE3PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import json\n",
        "\n",
        "#Load train captions and videos\n",
        "captions = json.load(open(\"/content/drive/MyDrive/CS230 Project/data/train_frames/captions.json\"))\n",
        "train_captions = []\n",
        "train_images = []\n",
        "for file in os.listdir(\"/content/drive/MyDrive/CS230 Project/data/train_frames\"):\n",
        "  if (file == 'captions.json'):\n",
        "    continue\n",
        "  train_images.append(file)\n",
        "  train_captions.append(captions[file.split(\"-\")[0]])\n",
        "\n",
        "#Load test captions and videos\n",
        "captions = json.load(open(\"/content/drive/MyDrive/CS230 Project/data/test_frames/captions.json\"))\n",
        "test_captions = []\n",
        "test_images = []\n",
        "for file in os.listdir(\"/content/drive/MyDrive/CS230 Project/data/test_frames\"):\n",
        "  if (file == 'captions.json'):\n",
        "    continue\n",
        "  test_images.append(file)\n",
        "  test_captions.append(captions[file.split(\"-\")[0]])"
      ],
      "metadata": {
        "id": "Pr4bxz0kgpHk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Original CLIP Model*** "
      ],
      "metadata": {
        "id": "JfKNaIf5E9pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.7.1+cu110 torchvision  torchtext torchaudio --extra-index-url https://download.pytorch.org/whl/cu110\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtSuGwiRH4LY",
        "outputId": "f2828c4d-4ba7-4ce5-cd9a-0931301b4c44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu110\n",
            "Collecting torch==1.7.1+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n",
            "\u001b[K     |███████████████████████         | 834.1 MB 1.4 MB/s eta 0:03:53tcmalloc: large alloc 1147494400 bytes == 0x3a912000 @  0x7f267d4e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7 MB 1.4 MB/s eta 0:01:13tcmalloc: large alloc 1434370048 bytes == 0x7ef68000 @  0x7f267d4e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1156.7 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0xd4754000 @  0x7f267d4e5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1156.8 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 117.9 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.3-cp37-cp37m-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 87.9 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 48.3 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 449 kB/s \n",
            "\u001b[?25h  Downloading torchvision-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 91 kB/s \n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 27.1 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 12.0 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 24.7 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 6.3 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 42.8 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 27.5 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 27.8 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 28.1 MB/s \n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 41.4 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 44.9 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 45.2 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.3 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 49.6 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 32.4 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 39.1 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 49.0 MB/s \n",
            "\u001b[?25h  Downloading torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Installing collected packages: torch, torchvision, torchtext, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.11.0+cu113\n",
            "    Uninstalling torchaudio-0.11.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.11.0+cu113\n",
            "Successfully installed torch-1.7.1+cu110 torchaudio-0.7.2 torchtext-0.8.1 torchvision-0.8.2+cu110\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-fc4lw_4w\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-fc4lw_4w\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.7.1+cu110)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.8.2+cu110)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=fe6e003e608dfd14475a802c15d2bc0487f355135549fd965d05f1bcbea229ee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cichibto/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "o4J4qWmIH_ZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd5d7c0-b7e0-474c-dfff-38828f839415"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 96.3MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test CLIP image captioning (assign caption to given image)\n",
        "\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/CS230 Project/images/catimage.png\")).unsqueeze(0).to(device)\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
        "#image = preprocess(Image.open(\"/content/drive/MyDrive/CS230 Project/data/test_frames/\" + test_images[0])).unsqueeze(0).to(device)\n",
        "#text = clip.tokenize(test_captions[0:3]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    \n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu8_UzhoOfLs",
        "outputId": "78f7f7de-a25a-4976-9735-df51f5f9f4ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[0.00192544 0.00799205 0.99008256]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Finetuning on CLIP Model for Transfer Learning*** (adapted from https://github.com/openai/CLIP/issues/83)"
      ],
      "metadata": {
        "id": "JODMQZhzQ1Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch.utils.data as tor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "#Create custom PyTorch dataset containing images and corresponding captions\n",
        "\n",
        "#df = {'image': [\"https://www.freeiconspng.com/thumbs/cat-png/cat-png-17.png\", 'https://w7.pngwing.com/pngs/174/600/png-transparent-cat-animal-lovely-cat.png', 'https://www.pngmart.com/files/1/Dog-PNG-File.png', 'https://cdn.pixabay.com/photo/2020/06/08/22/50/dog-5276317_1280.png'],\n",
        "#      'caption': ['a cat', 'a cat', 'a dog', 'a dog']}\n",
        "\n",
        "df = {'image': train_images[:150], 'caption': train_captions[:150]}\n",
        "\n",
        "class internet_image_caption_dataset(tor.Dataset):\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.images = df[\"image\"]\n",
        "        self.caption = df[\"caption\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        response = requests.get(self.images[idx])\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        images = preprocess(img) #preprocess from clip.load\n",
        "        caption = self.caption[idx]\n",
        "        return images,caption\n",
        "\n",
        "class image_caption_dataset(tor.Dataset):\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.images = df[\"image\"]\n",
        "        self.caption = df[\"caption\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(\"/content/drive/MyDrive/CS230 Project/data/train_frames/\" + self.images[idx])\n",
        "        images = preprocess(img) #preprocess from clip.load\n",
        "        caption = self.caption[idx]\n",
        "        return images,caption\n",
        "\n",
        "dataset = image_caption_dataset(df)\n",
        "BATCH_SIZE = 50\n",
        "train_dataloader = tor.DataLoader(dataset,batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "jdjBmTt1uBpZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as tor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Train model\n",
        "\n",
        "'''\n",
        "BATCH_SIZE must larger than 1\n",
        "BATCH_SIZE = 2\n",
        "train_dataloader = tor.DataLoader(DataSet,batch_size = BATCH_SIZE) #Define your own dataloader\n",
        "'''\n",
        "\n",
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model): \n",
        "    for p in model.parameters(): \n",
        "        p.data = p.data.float() \n",
        "        p.grad.data = p.grad.data.float() \n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
        "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
        "if device == \"cpu\":\n",
        "  model.float()\n",
        "else :\n",
        "  clip.model.convert_weights(model)\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper\n",
        "\n",
        "EPOCH = 5\n",
        "for epoch in range(EPOCH):\n",
        "  print(\"Starting epoch \", epoch, \"...\" )\n",
        "  b = 1\n",
        "  for batch in train_dataloader:\n",
        "      print(\"Starting batch \", b)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      list_image,list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
        "      images = list_image\n",
        "      #images= torch.stack([preprocess(img) for img in list_image],dim=0).to(device) # omit the Image.fromarray if the images already in PIL format, change this line to images=list_image if using preprocess inside the dataset class\n",
        "      texts = clip.tokenize(list_txt).to(device)\n",
        "    \n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "      ground_truth = torch.arange(BATCH_SIZE,dtype=torch.long,device=device)\n",
        "\n",
        "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "      total_loss.backward()\n",
        "      if device == \"cpu\":\n",
        "         optimizer.step()\n",
        "      else : \n",
        "        convert_models_to_fp32(model)\n",
        "        optimizer.step()\n",
        "        clip.model.convert_weights(model)\n",
        "      b += 1"
      ],
      "metadata": {
        "id": "rWgznF1GLTKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f72f6e9-5846-4245-f4d7-9354a2b7737f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch  0 ...\n",
            "Starting batch  1\n",
            "Starting batch  2\n",
            "Starting batch  3\n",
            "Starting epoch  1 ...\n",
            "Starting batch  1\n",
            "Starting batch  2\n",
            "Starting batch  3\n",
            "Starting epoch  2 ...\n",
            "Starting batch  1\n",
            "Starting batch  2\n",
            "Starting batch  3\n",
            "Starting epoch  3 ...\n",
            "Starting batch  1\n",
            "Starting batch  2\n",
            "Starting batch  3\n",
            "Starting epoch  4 ...\n",
            "Starting batch  1\n",
            "Starting batch  2\n",
            "Starting batch  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save trained model\n",
        "torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss,\n",
        "        }, \"/content/drive/MyDrive/CS230 Project/model_ckpts/model.pt\") #just change to your preferred folder/filename"
      ],
      "metadata": {
        "id": "d9IhjGZfQwOv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation Metrics***"
      ],
      "metadata": {
        "id": "YZ1Ioe_zvODX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load trained model\n",
        "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/CS230 Project/model_ckpts/model.pt\")\n",
        "\n",
        "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
        "#checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
        "#checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
        "#checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "W89_2OEDQyAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43d1767-2df8-4dee-fcb7-cb6210457f23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Calculate train accuracy\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for i in range(150):\n",
        "  print(\"Done \", i + 1, \" out of 150\")\n",
        "  image = preprocess(Image.open(\"/content/drive/MyDrive/CS230 Project/data/train_frames/\" + train_images[i])).unsqueeze(0).to(device)\n",
        "  text = clip.tokenize(train_captions[:150]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image)\n",
        "      text_features = model.encode_text(text)\n",
        "    \n",
        "      logits_per_image, logits_per_text = model(image, text)\n",
        "      probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "      correct += np.max(probs, axis=1)[0] == probs[0,i]\n",
        "\n",
        "print(\"Train accuracy:\", correct/150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q64glOuiGCie",
        "outputId": "372e1c0a-54ee-4169-fcae-d97ae7468549"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done  1  out of 150\n",
            "Done  2  out of 150\n",
            "Done  3  out of 150\n",
            "Done  4  out of 150\n",
            "Done  5  out of 150\n",
            "Done  6  out of 150\n",
            "Done  7  out of 150\n",
            "Done  8  out of 150\n",
            "Done  9  out of 150\n",
            "Done  10  out of 150\n",
            "Done  11  out of 150\n",
            "Done  12  out of 150\n",
            "Done  13  out of 150\n",
            "Done  14  out of 150\n",
            "Done  15  out of 150\n",
            "Done  16  out of 150\n",
            "Done  17  out of 150\n",
            "Done  18  out of 150\n",
            "Done  19  out of 150\n",
            "Done  20  out of 150\n",
            "Done  21  out of 150\n",
            "Done  22  out of 150\n",
            "Done  23  out of 150\n",
            "Done  24  out of 150\n",
            "Done  25  out of 150\n",
            "Done  26  out of 150\n",
            "Done  27  out of 150\n",
            "Done  28  out of 150\n",
            "Done  29  out of 150\n",
            "Done  30  out of 150\n",
            "Done  31  out of 150\n",
            "Done  32  out of 150\n",
            "Done  33  out of 150\n",
            "Done  34  out of 150\n",
            "Done  35  out of 150\n",
            "Done  36  out of 150\n",
            "Done  37  out of 150\n",
            "Done  38  out of 150\n",
            "Done  39  out of 150\n",
            "Done  40  out of 150\n",
            "Done  41  out of 150\n",
            "Done  42  out of 150\n",
            "Done  43  out of 150\n",
            "Done  44  out of 150\n",
            "Done  45  out of 150\n",
            "Done  46  out of 150\n",
            "Done  47  out of 150\n",
            "Done  48  out of 150\n",
            "Done  49  out of 150\n",
            "Done  50  out of 150\n",
            "Done  51  out of 150\n",
            "Done  52  out of 150\n",
            "Done  53  out of 150\n",
            "Done  54  out of 150\n",
            "Done  55  out of 150\n",
            "Done  56  out of 150\n",
            "Done  57  out of 150\n",
            "Done  58  out of 150\n",
            "Done  59  out of 150\n",
            "Done  60  out of 150\n",
            "Done  61  out of 150\n",
            "Done  62  out of 150\n",
            "Done  63  out of 150\n",
            "Done  64  out of 150\n",
            "Done  65  out of 150\n",
            "Done  66  out of 150\n",
            "Done  67  out of 150\n",
            "Done  68  out of 150\n",
            "Done  69  out of 150\n",
            "Done  70  out of 150\n",
            "Done  71  out of 150\n",
            "Done  72  out of 150\n",
            "Done  73  out of 150\n",
            "Done  74  out of 150\n",
            "Done  75  out of 150\n",
            "Done  76  out of 150\n",
            "Done  77  out of 150\n",
            "Done  78  out of 150\n",
            "Done  79  out of 150\n",
            "Done  80  out of 150\n",
            "Done  81  out of 150\n",
            "Done  82  out of 150\n",
            "Done  83  out of 150\n",
            "Done  84  out of 150\n",
            "Done  85  out of 150\n",
            "Done  86  out of 150\n",
            "Done  87  out of 150\n",
            "Done  88  out of 150\n",
            "Done  89  out of 150\n",
            "Done  90  out of 150\n",
            "Done  91  out of 150\n",
            "Done  92  out of 150\n",
            "Done  93  out of 150\n",
            "Done  94  out of 150\n",
            "Done  95  out of 150\n",
            "Done  96  out of 150\n",
            "Done  97  out of 150\n",
            "Done  98  out of 150\n",
            "Done  99  out of 150\n",
            "Done  100  out of 150\n",
            "Done  101  out of 150\n",
            "Done  102  out of 150\n",
            "Done  103  out of 150\n",
            "Done  104  out of 150\n",
            "Done  105  out of 150\n",
            "Done  106  out of 150\n",
            "Done  107  out of 150\n",
            "Done  108  out of 150\n",
            "Done  109  out of 150\n",
            "Done  110  out of 150\n",
            "Done  111  out of 150\n",
            "Done  112  out of 150\n",
            "Done  113  out of 150\n",
            "Done  114  out of 150\n",
            "Done  115  out of 150\n",
            "Done  116  out of 150\n",
            "Done  117  out of 150\n",
            "Done  118  out of 150\n",
            "Done  119  out of 150\n",
            "Done  120  out of 150\n",
            "Done  121  out of 150\n",
            "Done  122  out of 150\n",
            "Done  123  out of 150\n",
            "Done  124  out of 150\n",
            "Done  125  out of 150\n",
            "Done  126  out of 150\n",
            "Done  127  out of 150\n",
            "Done  128  out of 150\n",
            "Done  129  out of 150\n",
            "Done  130  out of 150\n",
            "Done  131  out of 150\n",
            "Done  132  out of 150\n",
            "Done  133  out of 150\n",
            "Done  134  out of 150\n",
            "Done  135  out of 150\n",
            "Done  136  out of 150\n",
            "Done  137  out of 150\n",
            "Done  138  out of 150\n",
            "Done  139  out of 150\n",
            "Done  140  out of 150\n",
            "Done  141  out of 150\n",
            "Done  142  out of 150\n",
            "Done  143  out of 150\n",
            "Done  144  out of 150\n",
            "Done  145  out of 150\n",
            "Done  146  out of 150\n",
            "Done  147  out of 150\n",
            "Done  148  out of 150\n",
            "Done  149  out of 150\n",
            "Done  150  out of 150\n",
            "Train accuracy: 0.6866666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate test accuracy\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for i in range(50):\n",
        "  print(\"Done \", i + 1, \" out of 50\")\n",
        "  image = preprocess(Image.open(\"/content/drive/MyDrive/CS230 Project/data/test_frames/\" + test_images[i])).unsqueeze(0).to(device)\n",
        "  text = clip.tokenize(test_captions[:50]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image)\n",
        "      text_features = model.encode_text(text)\n",
        "    \n",
        "      logits_per_image, logits_per_text = model(image, text)\n",
        "      probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "      correct += np.max(probs, axis=1)[0] == probs[0,i]\n",
        "\n",
        "print(\"Test accuracy:\", correct/50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUnPHK1PFtnG",
        "outputId": "70066e39-04e7-4af1-faf6-ad8da51f5134"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done  1  out of 50\n",
            "Done  2  out of 50\n",
            "Done  3  out of 50\n",
            "Done  4  out of 50\n",
            "Done  5  out of 50\n",
            "Done  6  out of 50\n",
            "Done  7  out of 50\n",
            "Done  8  out of 50\n",
            "Done  9  out of 50\n",
            "Done  10  out of 50\n",
            "Done  11  out of 50\n",
            "Done  12  out of 50\n",
            "Done  13  out of 50\n",
            "Done  14  out of 50\n",
            "Done  15  out of 50\n",
            "Done  16  out of 50\n",
            "Done  17  out of 50\n",
            "Done  18  out of 50\n",
            "Done  19  out of 50\n",
            "Done  20  out of 50\n",
            "Done  21  out of 50\n",
            "Done  22  out of 50\n",
            "Done  23  out of 50\n",
            "Done  24  out of 50\n",
            "Done  25  out of 50\n",
            "Done  26  out of 50\n",
            "Done  27  out of 50\n",
            "Done  28  out of 50\n",
            "Done  29  out of 50\n",
            "Done  30  out of 50\n",
            "Done  31  out of 50\n",
            "Done  32  out of 50\n",
            "Done  33  out of 50\n",
            "Done  34  out of 50\n",
            "Done  35  out of 50\n",
            "Done  36  out of 50\n",
            "Done  37  out of 50\n",
            "Done  38  out of 50\n",
            "Done  39  out of 50\n",
            "Done  40  out of 50\n",
            "Done  41  out of 50\n",
            "Done  42  out of 50\n",
            "Done  43  out of 50\n",
            "Done  44  out of 50\n",
            "Done  45  out of 50\n",
            "Done  46  out of 50\n",
            "Done  47  out of 50\n",
            "Done  48  out of 50\n",
            "Done  49  out of 50\n",
            "Done  50  out of 50\n",
            "Test accuracy: 0.08\n"
          ]
        }
      ]
    }
  ]
}